---
title: "How GPT Predicts the Next Token: Theory and Code"
mathjax: true
---

# üß† Theoretical Foundation: Causal Language Modeling with GPT

## Objective

A GPT model is trained to **predict the next token** in a sequence:

$$
L = \sum_{t=1}^{T} \log P(u_t \mid u_0, \dots, u_{t-1}; \theta)
$$

- Each token $ u_t $ is predicted **based only on previous tokens** (causality).
- The model outputs a distribution over the vocabulary for **every time step**, but:
  - During **training**, predictions are made at all positions.
  - During **inference**, **only the last token** matters ‚Äî it's used to generate the next.

---

# üß± Architecture Details

## Input Embedding

The input is constructed by summing token embeddings and positional encodings:

$$
h_0 = W_e[U] + W_p
$$

- $ U \in \mathbb{R}^{b \times k} $, $$ W_e[U]$$ is advanced index and results shape $ [b, k, d]$ 
- $ W_e \in \mathbb{R}^{V \times d} $: token embedding matrix
- $ W_p \in \mathbb{R}^{k \times d} $: positional encoding
- $ h_0 \in \mathbb{R}^{b \times k \times d} $: input to transformer layers

## Transformer Stack

The transformer decoder stack maps $ h_0 $ to output representations:

$$
h_n = \text{Transformer}(h_0)
$$

Note that $i$th token only attends to tokens $\le t$. 

```python
import torch
import torch.nn.functional as F

# Simulated attention scores
seq_len = 5
d_k = 64
torch.manual_seed(0)

# (batch_size=1, heads=1 for simplicity)
Q = torch.randn(1, seq_len, d_k)
K = torch.randn(1, seq_len, d_k)
V = torch.randn(1, seq_len, d_k)

# Raw attention scores
attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)  # shape: (1, seq_len, seq_len)

# Create causal mask (upper triangle masked out)
causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(torch.bool)  # (seq_len, seq_len)

### --------- ‚úÖ Correct Method: Add -inf before softmax --------- ###
masked_scores_correct = attn_scores.clone()
masked_scores_correct[~causal_mask] = float('-inf')  # mask future tokens

attn_probs_correct = F.softmax(masked_scores_correct, dim=-1)

### --------- ‚ùå Incorrect Method: Multiply by 0/1 mask --------- ###
masked_scores_wrong = attn_scores.clone()
masked_scores_wrong = masked_scores_wrong * causal_mask.float()  # 0 out future tokens

attn_probs_wrong = F.softmax(masked_scores_wrong, dim=-1)

### --------- Print Comparison --------- ###
print("Original attention scores:\n", attn_scores[0])
print("\n‚úÖ Correct masked attention probs:\n", attn_probs_correct[0])
print("\n‚ùå Incorrect masked attention probs:\n", attn_probs_wrong[0])

```

- Output: $ h_n \in \mathbb{R}^{b \times k \times d} $

## Output Logits

The final logits are computed as:

$$
\text{logits} = h_n W_e^T \in \mathbb{R}^{b \times k \times V}
$$

- Softmax is applied over the vocabulary axis
- During inference, only the final step is used:

$$
\text{logits}_{\text{next}} = \text{logits}[:, -1, :]
$$


```python
def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0,
                    device='cuda', top_p=0, stop_token=[]):
    if start_token is None:
        assert context is not None, 'Specify exactly one of start_token and context!'
        context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)
    else:
        assert context is None, 'Specify exactly one of start_token and context!'
        context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)
    prev = context
    output = context
    past = None

    count = 0
    with torch.no_grad():
        while count < length:
            logits, past = model(prev, past=past)
            logits = logits[:, -1, :] / temperature
            logits = top_k_top_p_filtering(logits, top_p=top_p, top_k=top_k)
            probs = F.softmax(logits, dim=-1)
            prev = torch.multinomial(probs, num_samples=1)
            output = torch.cat((output, prev), dim=1)
            count += 1
            if prev in stop_token:
                break
    return output
```

References
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research/gpt/language_understanding_paper.pdf)
- [Next Token Prediction](https://github.com/JasonBenn/duet/blob/master/generate.py#L55)
